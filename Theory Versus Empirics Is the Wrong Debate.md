# The Dispute: What Are We Actually Disagreeing About?

![](asset/Pasted%20image%2020251223141042.png)

I recently had a heated but meaningful discussion with a close friend. This document is an attempt to clarify where our disagreement actually lies—and where it does not.
### Andy’s Position

For clarity, I will refer to my friend as Andy.
Andy’s main claims are as follows:

1. In his region, undergraduate students are required to publish or complete a thesis as a graduation requirement.  
   This institutional design results in a large volume of low-quality academic output—a product of institutional design.
2. Favors empirical, hands-on, and industry-adjacent approaches over theoretical work, emphasizing clear metrics and tangible output.
### My position
I agree point 1. core values: 

> “When a measure becomes a target, it ceases to be a good measure.”, by Charles Goodhart — Goodhart’s Law

1. In this case, the measure is the production of the research paper
2. Empirical methods, as an approach, are not intrinsically superior to theoretical research. Specifically, how empirical research can be flawed and counterproductive in cases. 
3. Theoretical Academia is no way less than Empiricial Research

# What Does “Empiricism” Mean?

### Empirical Research versus Co-op
In this discussion, Andy appears to use “empirical” in a way that closely overlaps with Co-op or industry-oriented practice. When I was speaking, I have *empirical research* in minds.

To avoid confusion, it is important to distinguish between the following concepts:

- **Empirical academia**:  
  Research conducted *within the academic system* using data, experiments, measurement, or observation to test, validate, or refine theories.
- **Industry practice / industrial collaboration (Co-op)**:  
  Work focused on engineering deployment, commercial value, operational efficiency, or real-world product impact.

These two are related but not equivalent.

### Empirical Research
![](asset/Pasted%20image%2020251223141404.png)
Within academia, empirical research refers to a **methodological distinction**, not a connection to industry.

> **Empirical research in academia** means using data, experiments, econometrics, or observation *within the academic system* to test or revise theoretical claims.

Examples include:
- **Economics**: Testing models using causal inference methods
- **Psychology**: Controlled experiments and psychometric data
- **Physics**: Experimental verification of theoretical predictions

This represents a *theory-versus-empirics* distinction **inside academia**, not a contrast between academia and industry.

It is therefore different from industry practice, which prioritizes:
- Engineering feasibility
- Commercial value
- Revenue, efficiency, and deployment

#### Typical Workflow of an Empirical Academic Paper

A typical empirical research workflow includes:

- Proposing a research question and identification strategy  
  (e.g., natural experiments, Difference-in-Differences)
- Acquiring or constructing datasets
- Estimation, robustness checks, and interpretation
- Publication, followed by replication attempts, critiques, and re-analysis

### Conclude: “Industrial” versus “Empirical”

Andy’s usage of “empirical” appears rooted in his disciplinary experience. In fields such as Mechanical Engineering, knowledge is deeply grounded in physical experimentation, and cooperative education (co-op) or industry collaboration is common. In that sense, theory and experiment are tightly coupled.

This perspective is entirely reasonable. And our misalignment comes from using the using same terms for Co-op or industry collaboration and empirical research.  

This clarification allows me to focus on empirical research within the academic system, and on the **structural reasons for skepticism toward its current incentive-driven practice**.

> Science is the belief in the ignorance of experts. -- Richard Feynman

# Empirical Research: Data Is Not Equivalent to Truth
### How Empirical Research Can Be Manipulated (Mechanisms)

Empirical research is not inherently unreliable—but it is vulnerable at multiple stages.

Common problematic practices include:
- **p-hacking**: Running multiple analyses or specifications until statistically significant results appear
- **Publication bias**: Positive or “interesting” results are more likely to be published
- **Data dredging**: Searching through data without a pre-specified hypothesis
- **Fabricated or manipulated datasets**

Researchers may manipulate results at various stages:
- **Data collection**: Fabrication, selective inclusion or exclusion
- **Statistical analysis**:  
  Changing covariates, clustering levels, or test directions  
  Re-running tests until *p* < 0.05
- **Treatment definition**:  
  Redefining when treatment starts or binarizing continuous variables
- **Model selection**: Specification fishing (a core contributor to the replication crisis)

In short, empirical research can be manipulated at every layer:
> **data → variables → models → statistics → narratives → incentives**
### Case Studies 
![](asset/Pasted%20image%2020251223141318.png)
My claim is not that empirical research is useless, but that documented failures and distortions exist across disciplines, often driven by incentive misalignment.

Examples include:
1. Economics
	- *“Artificial Intelligence, Scientific Discovery, and Product Innovation”* (2025): concerns over data provenance
	- Reinhart & Rogoff (2010), *Growth in a Time of Debt*: selective data exclusion leading to misleading conclusions, later used to justify austerity policies in the US and Europe
2. Psychology
	- Cyril Burt’s twin studies on IQ heritability
	- James Vicary’s subliminal advertising “experiment”
3. Life and Physical Sciences
	- Andrew Wakefield (1998): fabricated data and undisclosed conflicts of interest in vaccine-autism research
	- Jan Hendrik Schön (2000–2002): direct data fabrication detected through identical noise patterns across experiments
### Recommended Channel  for more detailed.
To better understand how academic incentives can distort research behavior—particularly in empirical and data-driven fields—I recommend the following commentators, who discuss these issues from different disciplinary and institutional perspectives.

### 1. **Sabine Hossenfelder**
Sabine Hossenfelder, a theoretical physicist, frequently critiques structural problems in modern academia, including **publish-or-perish incentives**, pressure for **sensational results**, and the **misapplication or erosion of scientific methodology**. Her work is especially relevant to the themes discussed in this article.

**Selected videos most relevant to this discussion:**

1. **“I can’t believe this really happened.”**  
    Explores concrete cases where academic norms and incentives led to surprising or troubling outcomes.
2. **“My dream died, and now I’m here.”**  
    Focuses on research funding structures, overhead costs, and how academic incentives can gradually **erode creativity, curiosity, and intrinsic motivation**.
3. **“The crisis in physics is real: Science is failing.”**  
    Discusses methodological stagnation, community reinforcement of weak ideas, and how funding mechanisms can unintentionally reward conformity over rigor.

### 2. **Pete Judo**
Pete Judo examines systemic weaknesses in academic research from a science-communication and meta-research perspective, with particular attention to **peer review**, **publication incentives**, and **scientific misconduct**.

**Relevant videos include:**

1. **“The BROKEN system at the heart of Academia.”**  
    Analyzes peer review as noisy, inconsistent, and sometimes biased, highlighting how prestige, conflicts of interest, and cost structures can undermine reliability.  
    Includes discussion of high-profile cases such as Francesca Gino, involving data manipulation and hypothesis-conforming alterations.
2. **“How to catch a bad scientist.”**  
    Examines warning signals such as unusually high publication volume, persistent “too-good-to-be-true” results, and undisclosed conflicts of interest.
3. **“ACADEMIA IS BROKEN! Stanford Nobel-Prize Scandal Explained.”**  
    Uses a prominent case to illustrate how institutional prestige and incentive misalignment can delay scrutiny and accountability.

>My claim is not that empirical or practice-oriented research is inherently flawed.  
>Rather, empirical / 實證派 research can _appear_ especially productive and useful under current incentive structures, while in some cases producing **misleading or counterproductive outcomes** when validation, replication, and methodological rigor are subordinated to performance metrics.

# The utility of Theoretical academia
![](asset/Pasted%20image%2020251223141208.png)
### Ideas Once Considered “Useless”
 Theoretical academia is often dismissed as “useless” when judged by immediate, short-term utility. This judgment, however, is **fundamentally shortsighted**. Many theoretical contributions that initially appeared detached from practical application later became the **foundations of entire industries and technological systems**.

A few representative examples illustrate this pattern:

- **Number theory → modern cryptography**  
    In the early 20th century, number theory was widely regarded as a purely abstract discipline with no practical value.  
    **G. H. Hardy** famously declared that his work had “no practical use whatsoever.”  
    Yet today, RSA, public-key cryptography, HTTPS, blockchain systems, and electronic payments—all pillars of the digital economy—are built directly on number-theoretic foundations.  
    Had number theory been evaluated in 1910 by the criterion of “industrial usefulness,” much of modern cybersecurity would never have existed.
    
- **Computability theory → modern computers**  
    The work of **Alan Turing** and Alonzo Church on computability and formal logic initially appeared as abstract logical exercises.  
    The Turing Machine, at the time, had no hardware, no programs, and no apparent real-world function.  
    Yet these theoretical models became the conceptual blueprint for all modern computing systems.
    
- **Economics: abstract theory → real-world mechanisms**  
    Game theory, once viewed as highly abstract, now underpins auction design, bidding strategies, and modern market design.  
    Similarly, Difference-in-Differences was largely theoretical until large-scale administrative data and natural policy experiments made its practical application feasible.
    
- **Computer architecture and AI**  
    Ideas such as Amdahl’s Law (associated with **Gene Amdahl**) and multi-core CPU designs were long criticized as “academic over-engineering.”  
    Likewise, the Transformer architecture—introduced in the 2017 paper _Attention Is All You Need_—was initially a theoretical modeling choice, later becoming the backbone of modern large-scale AI systems.
    
These contributions were **not immediately recognized** as useful. Their value emerged only after complementary technologies, data, or infrastructure matured.

### Why Theory Matters Structurally
![](asset/Pasted%20image%2020251223141231.png)

>Industry does not systematically invest in non-commercialized foundational theory.

Industry rarely spends resources validating non-commercialized ideas, exploring abstract conceptual spaces, or developing tools whose payoff may take decades to materialize. That role belongs almost exclusively to academia.

Theoretical academia provides the **conceptual infrastructure** that later enables practical progress. Tools and frameworks such as:

- A/B testing
- Causal inference
- Reinforcement learning

All of these originated as academic abstractions before becoming standard instruments in industry. In practice, software engineers, data scientists, and system designers routinely read academic papers in order to implement, adapt, or operationalize them. Conversely, some engineers return to academia with industrial experience, bringing back practical constraints, empirical insights, and new problem formulations that enrich theoretical research.

>Theory does not compete with practice; **it precedes and enables it**.  
>**Empirical research does not replace theory; it tests, refines, and sometimes corrects theoretical claims.**

# Why Empirical Work Is Currently Favored (Incentives)

Under contemporary academic conditions, empirical research is often **more favored, more achievable, and more “fashionable”**, and investment in it is therefore perceived as more immediately rewarding.
Several structural factors contribute to this preference:

1. Institutional incentives
Journals tend to prioritize data-backed claims; funding agencies emphasize “measurable impact”; and hiring or promotion committees favor clear, countable outputs such as tables, plots, and regression results.
2. **Perceived objectivity and communicability**  
Empirical work _appears_ objective and aligns well with policy discourse, industry interests, and “evidence-based” narratives. As a result, it is often easier to justify to administrators, policymakers, and non-specialist audiences.
3. **Lower barriers to entry and faster turnaround**  
Researchers can download existing datasets, apply standardized analytical pipelines, and produce publishable results within relatively short time frames. In contrast, theoretical research often requires years of immersion, deep mathematical or conceptual breakthroughs, and carries a higher risk of producing work that is intellectually interesting but difficult to publish.

| Aspect               | Empirical Research | Theoretical Research |
| -------------------- | ------------------ | -------------------- |
| Time to first result | Short              | Long                 |
| Entry barrier        | Lower              | Very high            |
| Tooling              | Standardized       | Often bespoke        |
| Collaboration        | Relatively easy    | Often difficult      |
| Publishability       | Higher             | Riskier              |
# Conclusion: When KPI becomes the Goal
![](asset/Pasted%20image%2020251223141258.png)
Andy and I agree on the diagnosis: institutional incentives matter, and poorly designed KPIs distort behavior.

Where this article draws a sharper boundary is in rejecting the assumption that empirical output is inherently safer, more useful, or more truthful than theoretical work. Empirical research without rigor is not scientific practice—it is production. Theory without immediate application is not useless—it is delayed.

Scientific progress is not driven by whichever approach produces results fastest, but by the integrity of the feedback loop between abstraction, observation, and correction. Break that loop—by rewarding volume over validation—and both theory and empiricism degrade.

The danger, then, is not academic theory, nor empirical methods, nor even institutional pressure in isolation. The danger is confusing **measurement with meaning**.

When KPIs become goals, science does not simply slow down.  
It starts optimizing for the wrong reality.